{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV9xpc9ywH7r"
      },
      "source": [
        "# NLP : Analyse des sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-PNjfmIwFkP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhlxiFMHwdME",
        "outputId": "bbe38754-6ce2-42dc-9e35-be8a4c0fea15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "x_train = pd.read_csv(\"/content/drive/MyDrive/Train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTD4DQr8wjW0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def supprimer_crochets(text):\n",
        "  return re.sub('\\[[^]]*\\]', '', text)\n",
        "x_train['text']=x_train['text'].apply(supprimer_crochets)\n",
        "def supprimer_special(text, remove_digits=True):\n",
        " pattern=r'[^a-zA-z0-9\\s]'\n",
        " text=re.sub(pattern,'',text)\n",
        " return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGvAXbRfw6Aa",
        "outputId": "d2154e7f-1c22-4ac8-f9f4-062ff4df93f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download([\"names\",\"stopwords\",\"punkt\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo0ntqPFwocc"
      },
      "outputs": [],
      "source": [
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        " ps=nltk.porter.PorterStemmer()\n",
        " text= ' '.join([ps.stem(word) for word in text.split()])\n",
        " return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0OO7ttfwvZE"
      },
      "outputs": [],
      "source": [
        "#Apply function on review column\n",
        "x_train['text']=x_train['text'].apply(simple_stemmer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0q48bbSxBoq"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer=ToktokTokenizer()\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAydao0ZxFK7",
        "outputId": "2183bbd6-c33f-48c8-a183-c57681ef4956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'your', \"you're\", 'are', \"don't\", 'same', 'nor', \"needn't\", 'had', 'by', 'was', \"hadn't\", 'during', 'have', 'were', 'too', 'll', 'ain', 'her', 'has', 'more', 'in', 'to', 'out', 'me', 'which', 'doing', 'an', 'with', 'further', 'than', 'them', 'themselves', 'ourselves', 'each', 'been', 'down', 'isn', 'through', 'above', 'these', 'm', \"shouldn't\", 'i', 'only', 'between', 'ma', 'into', 'of', 'ours', 'because', \"she's\", 'don', 'itself', 'and', \"haven't\", 'few', 'not', 'hasn', \"you've\", 'does', 'their', 'she', 'couldn', 'wouldn', 'about', 'a', 'own', 'for', \"you'd\", 'you', 'being', 'here', 'myself', \"won't\", 'herself', 'the', 'both', 'no', 'what', \"doesn't\", 'yourselves', 'its', 'didn', 'up', \"mightn't\", 'will', 'weren', 'hadn', 'before', \"weren't\", 'wasn', 'again', 'as', 'but', 'that', \"you'll\", 'this', 'him', 'yours', \"wasn't\", 'd', 'how', 'won', 'just', 'so', 'while', \"shan't\", 'why', \"should've\", 'where', 'my', 'such', 'when', 'do', \"wouldn't\", 's', 'they', 'those', 'under', 'aren', 'haven', 'mightn', 'all', 're', 'over', 'he', 'there', 'can', 'whom', 'mustn', 'y', 'shan', 'at', 'hers', 'himself', \"aren't\", 'be', 'our', \"hasn't\", 'most', 'o', \"isn't\", 'needn', 'his', 'on', 'we', \"didn't\", 'theirs', \"it's\", 'or', 't', 'if', 'shouldn', 'then', 'having', 'until', 'once', 'now', 've', 'very', \"mustn't\", 'from', 'against', 'doesn', \"that'll\", \"couldn't\", 'below', 'after', 'is', 'did', 'off', 'am', 'other', 'who', 'any', 'some', 'should', 'it', 'yourself'}\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xtZBN3gxIMU"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        " tokens = tokenizer.tokenize(text)\n",
        " tokens = [token.strip() for token in tokens]\n",
        " filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        " filtered_text = ' '.join(filtered_tokens)\n",
        " return filtered_text\n",
        "x_train['text']=x_train['text'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "9PSichMRxL5r",
        "outputId": "0be98782-c24e-41c1-ae69-fc9ca8f2fae0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'grew ( b. 1965 ) watch love thunderbirds. mate school watched. play \" thunderbirds \" befor school , dure lunch school. want virgil scott. one want alan. count 5 becam art form. took children see movi hope would get glimps love child. bitterli disappointing. onli high point wa snappi theme tune. could compar origin score thunderbirds. thank earli saturday morn one televis channel still play rerun seri gerri anderson hi wife created. jonatha frake hand hi director chair , hi version wa complet hopeless. wast film. utter rubbish. cgi remak may accept replac marionett homo sapien subsp. sapien wa huge error judgment .'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#30000 commentaires pour l'entrainement\n",
        "norm_train_text=x_train.text[:30000]\n",
        "norm_train_text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "CnoEX0CCxOKt",
        "outputId": "d8f5b023-6da9-4569-fb3f-32a4ee4a1a41"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"giorgino long , excruci journey bad wors life protagonist movi named. young demobilized , gas-poison first world war lieuten veri delic health , previous wa doctor orphanag children mental deprivations , goe search new locat find much ever intend quit differ nature. depress atmospher ultim despair , insan one much less horribl sane , mad kind poetry , hold hypnot veri first frame thi film last. beauty : beauti winter mountains , beauti snowi landscap wild woods , ' even know sorrow sad last day war could made beautiful .\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#10000 commentaires pour le test\n",
        "norm_test_text=x_train.text[30000:]\n",
        "norm_test_text[35005]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BJrSycDxT94",
        "outputId": "64431089-5d04-4ed9-944a-a7c91b77d43d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tfidf_train: (30000, 79233)\n",
            "Tfidf_test: (10000, 79233)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#Tfidf vectorizer\n",
        "tv=TfidfVectorizer()\n",
        "#transformed train reviews\n",
        "tv_train_text=tv.fit_transform(norm_train_text)\n",
        "#transformed test reviews\n",
        "tv_test_text=tv.transform(norm_test_text)\n",
        "print('Tfidf_train:',tv_train_text.shape)\n",
        "print('Tfidf_test:',tv_test_text.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAY7Fd0qxXBs"
      },
      "outputs": [],
      "source": [
        "sentiment_data = x_train['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyV80pDlxZgk",
        "outputId": "09ceae3d-966b-4f36-c3fc-8a8975ed008d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        1\n",
            "        ..\n",
            "29995    1\n",
            "29996    1\n",
            "29997    1\n",
            "29998    0\n",
            "29999    1\n",
            "Name: label, Length: 30000, dtype: int64\n",
            "30000    0\n",
            "30001    1\n",
            "30002    0\n",
            "30003    0\n",
            "30004    0\n",
            "        ..\n",
            "39995    1\n",
            "39996    1\n",
            "39997    0\n",
            "39998    1\n",
            "39999    1\n",
            "Name: label, Length: 10000, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Spliting the sentiment data (labels)\n",
        "train_sentiments=sentiment_data[:30000]\n",
        "test_sentiments=sentiment_data[30000:]\n",
        "print(train_sentiments)\n",
        "print(test_sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqKDe8Woxb20",
        "outputId": "84f2bad5-0452-487f-b69d-0fae0aa3e6cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=1, max_iter=500)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "#training the model\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1)\n",
        "#Fitting the model for tfidf features\n",
        "lr_tfidf=lr.fit(tv_train_text,train_sentiments)\n",
        "print(lr_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA441NvAxdsJ",
        "outputId": "0fa6c68c-df0c-48d3-c0f9-9a147d0c0813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 ... 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "#Predicting the model for tfidf features\n",
        "lr_tfidf_predict=lr.predict(tv_test_text)\n",
        "print(lr_tfidf_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG6TaAslxfXu",
        "outputId": "93ab2533-ceb9-4e2a-d7e2-2cdabe5ac73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_tfidf_score : 0.8918\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
        "print(\"lr_tfidf_score :\",lr_tfidf_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yBL6SWhxpBd",
        "outputId": "7fa8d97c-2b61-4268-f916-a99c69345db9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[4397,  575],\n",
              "       [ 507, 4521]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusion_matrix(test_sentiments,lr_tfidf_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x47r5KysxpUV",
        "outputId": "02b7fdfb-1cca-4d4b-a23e-70e8ec186268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1]\n"
          ]
        }
      ],
      "source": [
        "#one comment prediction\n",
        "liste = []\n",
        "commentaire = \"hello it is a bad movie\"\n",
        "commentaire = supprimer_crochets(commentaire)\n",
        "commentaire = supprimer_special(commentaire)\n",
        "commentaire = simple_stemmer(commentaire)\n",
        "tokens = remove_stopwords(commentaire)\n",
        "liste.append(tokens)\n",
        "x = tv.transform(liste)\n",
        "res = lr.predict(x)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(lr_tfidf, 'model/sentiment_model.pkl')\n",
        "\n",
        "# Save the TfidfVectorizer\n",
        "joblib.dump(tv, 'model/tfidf_vectorizer.pkl')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
